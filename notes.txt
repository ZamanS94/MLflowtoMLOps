# Principles of MLOps
Transition friction, Version Control, Performance, Automation, Monitoring, Continuous training 
# MLflow
Open Source platform to mainatain end to end machine learning life cycle. 
# MLFlow components
## Tracking - Track expreriments to record and compare parameters and results
## Projects - Packaage code to ensure reusability and reproducibility
## Model - Provides standard unit for packaging models
## Registry - Central model store for model versioning stage tarnsitions, annotations

# set_tracking uri() 
## set the default tracking uri for current run
## we can be empty string, will create then "mlruns" folder
## we can provide name folder 
## we can provide file path -> but only works for C drive 
## we can provide remote path
## we can provide databricks workspace

# get_tracking uri()
## gives only output 

# create_experiment()
## to create new experiment
## can provide name of the experiment
## can provide artifact_location paramater -> 
## can provide tags as dictionary of key value pairs for future usage
## return the string id 

# set_experiment()
## set already created experiment
## we can pass experiment name, experiment id
## return mlflow.entities.Experiment 

# start_run()
## starts new MLflow under in an experiment 
## we can provide run_id -> if need to utilize existing run
## we can provide experiment_id -> if we don't provide run_id
## we can provide run_name -> if we don't provide run_id
## we can provide nested run
## we can provide tags also
## we can provide description of any run
## it returns mlflow.ActiveRun object as a context manager
## if we use "with", don't need to end manually -> otherwise, we need to end it with end_run

# end_run()
## to end the start run function 

# active_run()
## returns which is active now
## takes no paramter but returns run object 

# last_active_run()
## recently active run, that ended
## takes no paramter but returns run object

# mlflow.log_param() # mlflow.log_params()

## to log hyperparamters as key value pair
## we can provide name and value of the parameter as key value pair / dictionary of key value pairs 

# mlflow.log_metric() # mlflow.log_metrics()

## we can provide name and value of the metric as key, value pair/ dictionary of key value pairs 
## we can also add step like epoch

# mlflow.log_artifact() # mlflow.log_artifacts()
## artifact can be anything -> it can be output model, training dataset
## we need to proivide local path -> path of the file we need to store
## artifact_path -> where we need to store the dataset

# mlflow.get_artifact_uri()
## returns absolute URI referring to the specified artifact or the currently active run's artifact root

# mlflow.set_tag() # mlflow.set_tags()
## tags are labels to find out specific experiment. it accept key value pair/ dictionary of key value pairs

# Multiple Run
## we need to complete multiple runs, in on etraining with different data slices
## for Hyperparameter Tuning, for Incremental Training, For Feature Engineeering, For Cross validation 


# Model Checkpoints
## we can use this to check performance of the model 

# If we use differernt experiments, we can keep same run name for each experiments

# Auto logging -> captures information in a automatic way -> should be done before the model training phase. 
# mlflow.autolog() -> will log for all the supported libraries # mlflow.<lib>.autolog() -> library specific logging 
## log_models -> bollean whether to log the model or not
## log_input_examples -> can be True to log the input examples from the training dataset 
## log_model_signatures -> Can be True to log the model signature/ model schema
## log_datasets -> can be True for logging dataset information
## disable -> can be True to disable automatic logging
## exclusive -> if True, content is not logged to user-created fluent runs 
## disable_for_unsupported_version -> can be True to disable autologging for incompatible versions that are not tested. 
## silent -> can be True, to supress all event logs and warnings from mlflow 

# for the mlflow.<lib>.autolog()
## we can use max_tuning_runs() -> set the max number of child mlflow runs, to analyze the runs effectively. 
## log_post_training_metrics -> can be True, to log post-training metrics like r2 score
## serialization_format -> model format. joblib for local model. couldpickle for the large distributed model -> can affect the performance 
## registered_model_name -> model name
## pos_label -> we can define the positive class label for binary classification 

# Tracking server
## Its a centralized repository, that stores metadata and atrifacts generated during the training session  

## The storage components -> actually store the artifacts and metadat
## Two types of Storage -> Backend Store and Artifact Store
## The Backend Store -> Stores exp name, id, run name, run id, paramters, metrics, tags-> DB Store: SQLite, MYSQL, PostgreSQL -> File Store: Amazon s3, Azure Blob storage, GCS
## Artifact Store -> Store artifacts like models, data, files

## The networking component -> Allows user to interact with the server using REST API or RPC. Depends on requirements. 
## Sometimes we have Proxy access -> because of security 

# Creating Tracking Server
mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlflow-artifacts --host 127.0.0.1 --port 5000












































